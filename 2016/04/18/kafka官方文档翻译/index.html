<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  




<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka官方文档翻译">
<meta property="og:url" content="http://yoursite.com/2016/04/18/kafka官方文档翻译/index.html">
<meta property="og:site_name" content="Stay hungry. Stay foolish.">
<meta property="og:description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">
<meta property="og:updated_time" content="2016-05-14T02:33:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kafka官方文档翻译">
<meta name="twitter:description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> Kafka官方文档翻译 | Stay hungry. Stay foolish. </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Stay hungry. Stay foolish.</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Kafka官方文档翻译
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-18T09:29:27+08:00" content="2016-04-18">
              2016-04-18
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本篇为Kafka官方文档翻译，加入了自己的理解。<br><a id="more"></a></p>
<h1 id="4-Design"><a href="#4-Design" class="headerlink" title="4 Design"></a>4 Design</h1><h2 id="4-1-Motivation"><a href="#4-1-Motivation" class="headerlink" title="4.1 Motivation"></a>4.1 Motivation</h2><p>我们设计Kafka的目的是为了提供一个能够处理实时数据的统一平台。为了实现这一目标，我们需要考虑很多使用场景。</p>
<p>它必须拥有高吞吐量（high-throughput），以支持规模庞大的数据流，例如实时日志的聚合。</p>
<p>它需要优雅地处理大量数据的保存（backlogs），以便支持周期性的从离线系统中加载数据。</p>
<p>它还要保证低延迟传输，满足传统消息系统的使用场景。</p>
<p>我们想让系统支持分区（partitioned）、分布式（distributed）等特性。这些特性促成了分区、消费模型等功能。</p>
<p>最后，当数据进来后，我们需要在机器出现故障时，系统具有容错性。</p>
<p>我们支持将这些使用场景分别对应独立的设计单元，使得它更像是一个数据库而不是消息系统。我们将会在下面的章节中概括一些设计元素。</p>
<h2 id="4-2-Persistence"><a href="#4-2-Persistence" class="headerlink" title="4.2 Persistence"></a>4.2 Persistence</h2><h3 id="Don’t-fear-the-filesystem"><a href="#Don’t-fear-the-filesystem" class="headerlink" title="Don’t fear the filesystem!"></a>Don’t fear the filesystem!</h3><p>Kafka非常依赖文件系统来保存或缓存数据。普遍观点认为磁盘速率很慢，这使得人们对持久化数据能够提供卓越的性能这一观点持怀疑态度。<strong>实际上，磁盘的数据有可能比我们想象的要慢，也可能比我们想象的要快，这取决于如何使用它，一个合理的磁盘结构设计通常比网络速率要快。</strong></p>
<p>磁盘性能的关键点在于磁盘驱动器的吞吐量，在过去的十多年里，这一指标不再由磁盘寻道时间决定。性能测试结果表明，在<a href="https://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank" rel="external">JBOD</a>结构的六块7200rpm SATA盘上，顺序写性能大约是600MB/sec，但是随机写只有大约100k/sec，相差6000倍。由于顺序读写在所有的使用场景中是最可能被预测的（most predictable），因此操作系统对它有很大的优化空间。现代操作系统提供了预读（read-ahead，预先从多个磁盘块读取数据）和后写（write-behind，合并小的逻辑写操作为一个大的物理写操作）机制。下面讨论的内容可以在<a href="http://queue.acm.org/detail.cfm?id=1563874" target="_blank" rel="external">ACM Queue article</a>找到，这篇文章说，在某些情况下，顺序的磁盘操作要比随机的内存操作快。</p>
<p>为了弥补磁盘和内存之间的性能差距，现在操作系统尽可能的利用主存来缓存磁盘数据。一个现代操作系统很乐意将所有的空闲内存都用作磁盘缓存，即使在内存回收（reclaimed）时会带来一定的性能损耗。所有的磁盘读写都要经过内存这一层缓存，除非使用直写I/O（direct I/O），所以，即使一个CPU在cache中保存了数据，操作系统也会将数据冗余存储在页缓存（pagecache）中，实现有效的冗余存储。</p>
<p>我们的程序运行在JVM之上，任何人在任何时候使用了JVM内存，都会有以下两个结果：</p>
<ol>
<li>对象的内存开销很大，经常是对象本身大小的两倍甚至更多</li>
<li>当堆中的数据增加时，垃圾回收变得非常繁琐缓慢</li>
</ol>
<p>由于这些原因，我们使用文件系统并依赖页缓存机制比把数据存储在内存要好一些。这样我们就拥有了两倍的缓存，并存储两份数据。在32GB内存的系统中，缓存容量可以达到28-30GB。并且，即使服务重启了，缓存还是会存储热数据，而CPU内的cache数据需要重新根据内存建立（10G的缓存大约需要10分钟），或者完全根据冷数据建立（这将严重影响初始性能）。这也极大简化了代码中关于维持缓存和文件系统一致性的相关逻辑，因为这些工作都交给了操作系统，这比程序自己去实现要更加高效正确。如果磁盘以线性读为主，那么每次从磁盘读到的数据缓存起来对效率的提升是非常有帮助的。</p>
<p>这些特性使得我们的设计非常简单：相比于把所有数据都存储在内存中，等到空间耗尽时再一次性刷入磁盘，我们恰好相反，所有的数据立即写入持久化日志中但不刷入磁盘，实际上数据只是传输到了内核的页缓存中。</p>
<h3 id="Constant-Time-Suffices"><a href="#Constant-Time-Suffices" class="headerlink" title="Constant Time Suffices"></a>Constant Time Suffices</h3><p>对于数据要持久化存储的消息系统，每个consumer都会分配一个queue，这个queue会关联一个BTree或其它能加速随机存取的数据结构。B树是最通用的数据结构，能在消息系统中支持事务、非事务性场景，但它带来的开销也很大。虽然B数的时间复杂度是O(log N)，通常认为O(log N)本质上等价于常数时间，但对于磁盘操作来说并非如此。磁盘寻道时间平均在10ms左右，每一个磁盘在一个时刻只能进行一个寻道操作，所以并行性受到了限制。所以少量的磁盘寻道也能造成非常大的开销。<strong>因为存储系统把快速的缓存操作和慢速的磁盘操作结合起来了，所以树结构的性能会随着数据量的增长呈超线性（superlinear）关系——数据量增长一倍，速度下降大于一倍。</strong></p>
<p>直觉上，一个持久化队列可以像日志记录解决方案（logging solutions）一样，实现为简单的从文件读或追加到文件。这种结构有一个优势就是所有操作都是O(1)，读不会阻塞写，反之也是。这是一个很明显的性能优势，因为性能完全和数据量解耦了，因此一台server能够充分利用廉价、低速的大容量（1+TB）SATA硬盘。虽然寻道操作性能低下，但对于大块数据的读写，这一时间比例会缩小，并且我们可以获得1/3的价格和3倍大的容量。</p>
<p>拥有一个很大的磁盘空间，并且性能不会随着空间增大而降低，这使得我们可以提供一些其它消息系统没有的特性。例如，不同于message消费完后就立刻被删除，我们能够将message保留相当长一段时间（例如一周），这给consumer带来了很大的灵活性，我们将会在下面描述。</p>
<blockquote>
<p>上面都是在说明Kafka的数据存储结构，为什么使用了简单的文件读写而没有使用BTree。一是空间性能比，二是持久化存储，可见文件存储也是Kafka的一大特性。</p>
</blockquote>
<h2 id="4-4-The-Producer"><a href="#4-4-The-Producer" class="headerlink" title="4.4 The Producer"></a>4.4 The Producer</h2><h3 id="Load-balancing"><a href="#Load-balancing" class="headerlink" title="Load balancing"></a>Load balancing</h3><p>Producer将数据直接发送给partition对应的leader，中间没有其它层。为了帮助producer完成发送，Kafka节点能够返回元数据给producer，内容包括哪些broker存活，topic的各个partition的leader是谁，以便producer将数据发送到正确的节点上。</p>
<p>Client端决定将数据发送到哪些partition。这可以通过随机算法或根据语义分区函数（semantic partitioning function）来定位partition。我们开放了语义分区相关的接口，让用户根据key映射partition（分区函数可以通过配置修改）。例如，key是用户ID，那么同一个用户的所有数据会打到同一个partition上，这反过来允许consumer对它们的消费做局部性假设（locality assumptions）。这样的partition分配策略允许consumer对局部敏感（locality-sensitive）的数据进行处理。</p>
<h3 id="Asynchronous-send"><a href="#Asynchronous-send" class="headerlink" title="Asynchronous send"></a>Asynchronous send</h3><p>Batching是提升性能的关键点。Kafka producer可以把数据聚集在内存中，然后一次性在一个request中发出去。可以设置最大聚集量和最大等待时间（例如64k或10ms）。Producer积累的数据越多，broker就具有更少的I/O次数，但一次I/O传输的数据量会更多。这一配置提供了一个在增大延迟和提高吞吐量之间的权衡。</p>
<h2 id="4-5-The-Consumer"><a href="#4-5-The-Consumer" class="headerlink" title="4.5 The Consumer"></a>4.5 The Consumer</h2><p>Consumer通过发送fetch请求来获取数据。Consumer在请求中指定log的offset，然后就能够收到从这个offset开始的数据了。Consumer对offset拥有控制权，因此可以通过操控offset来重复消费。</p>
<h3 id="Push-vs-pull"><a href="#Push-vs-pull" class="headerlink" title="Push vs. pull"></a>Push vs. pull</h3><p>我们需要考虑的一个首要问题是consumer是从broker拉（pull）数据还是broker推（push）数据到consumer。在这方面，Kafka遵从传统的设计，和大多数消息系统一样，producer push数据，consumer pull数据。但一些日志系统，例如Scribe和Apache Flume，是讲数据push到下游的。不论是push还是pull，都有各自的优缺点。对于基于push的系统，当consumer类型不同时，broker难以对不同consumer传输速率进行控制。Consumer的目标是尽可能快的消费到数据，但在基于push的系统中，当消费速率落后于生产速率时，consumer会被压垮（overwhelmed），这本质上是一种DOS攻击。相反，在基于pull的系统中，consumer能够根据自身情况调整消费速率。这种方法能够缓和退避协议（backoff protocol）带来的复杂性，退避协议通过consumer反馈是否过载来让broker控制传输速率。基于上述原因，我们选择pull模型。</p>
<p>基于pull系统的另一个优势是适合积累一定的数据后再传给consumer。在基于push的系统中，broker有两个选择：</p>
<ol>
<li>收到一条message就push给consumer</li>
<li>积累一定的message后再push给consumer</li>
</ol>
<p>方法1能够降低延迟但浪费带宽，方法2又无法知道下游的consumer能否立即处理这些批量数据。基于pull的系统能够应付这种情况：从当前offset一次性拉取所有（或设置一个拉取上限）有效数据。因此，这是批量拉取而不会引起高延迟的最佳方法。</p>
<p>原始的基于pull的系统有一个缺点，当broker没有数据时，consumer需要不断轮询是否有数据。为了避免这种忙等待，consumer采用了long poll机制，它会一直阻塞，直到broker积累了指定数量的数据才返回。</p>
<p>你可以想象另一种pull模型，producer把要发送的数据写到本地，然后broker去pull这些数据，最后consumer从broker pull下来。Store-and-forward模型就是这样一种方式（这种模型被用在了ActiveMQ中）。这种模式很有趣，但我们认为它不适用于我们的使用场景，即有上千数量的producer。实验证明，当producer规模很大时，把数据暂存在本地的方法并没有让系统更加可靠，并且运维成本不可估量。实际上可以利用一个pipeline和SLA从而避免producer本地存储。</p>
<h3 id="Consumer-Position"><a href="#Consumer-Position" class="headerlink" title="Consumer Position"></a>Consumer Position</h3><p>记录哪些数据被消费是一个影响消息系统性能的关键点（key performance points）。</p>
<p>很多消息系统把这部分数据存储在broker上。所以，当数据发送给consumer之后，broker要么立即记录这些数据的消费情况，要么等待consumer的响应后记录这些数据的消费情况。这是很符合直觉且务实的一种方法，broker知道哪些数据被消费了，然后将这些数据删除，从而控制数据规模。</p>
<p>但是，要让broker和consumer就哪些数据已经被消费达成一致并不是那么简单。如果broker每次下发数据后就立刻记录已消费情况，且consumer处理这条数据时发生错误（宕机或超时未收到数据等情况），那么这条message将会丢失。为了解决这个问题，许多消息系统增加了确认机制：消息发出后标记为<strong>sent</strong>，broker等待consumer的响应，收到响应后再将消息标记为<strong>consumed</strong>。这一策略修复了数据可能丢失的问题，但产生了一个新问题：</p>
<ol>
<li>consumer在收到数据之后，发送响应之前发生故障，那么这条message会被重复消费</li>
<li>broker必须存储每条message的多个状态，每条message都会执行下列动作：锁定为已发送-发送-标记为已消费-删除，影响性能</li>
</ol>
<p>还有一些其它棘手的问题需要解决，例如message发出后未收到consumer的响应。</p>
<p>Kafka处理这些问题的方法有所不同，我们的topic被拆分成全序的partition，每一个partition在任何时候都只能被一个consumer消费。这意味着每个partition的消费位置仅仅是一个整数，即下一个将要消费message的offset。这使得关于消费情况的状态量非常小，每个partition只是一个数字。这些状态量能够定期的作为checkpointed被存储起来。相比于通过consumer返回响应，这种方法开销更小。</p>
<blockquote>
<p>也就是说，Kafka不会出现consumer争用message的情况，修改offset不需要上锁，减少了系统开销和复杂度。</p>
</blockquote>
<p>这种方法还有一个附带好处，consumer能够回滚offset并重复消费。这有悖于queue的传统概念，但给consumer带来了很多重要的特性。例如，consumer消费了部分数据后发现程序有bug，那么待bug修复后，consumer可以重新消费这些数据。</p>
<h3 id="Offline-Data-Load"><a href="#Offline-Data-Load" class="headerlink" title="Offline Data Load"></a>Offline Data Load</h3><p>可伸缩的持久化（scalable persistence）允许consumer周期性地消费数据并将数据存储在离线系统中，例如Hadoop或关系型数据库。</p>
<p>在Hadoop中，数据的加载可以并行化，加载动作被映射到不同task中，每一个task根据三元组（node，topic，partition）并行地加载数据。Hadoop负责管理task，重启失效的task而不会重复消费——它只是简单地重启task并从最初的offset开始消费。</p>
<blockquote>
<p>这里的意思是，partition的设计能够让Hadoop并行地从broker拉取数据；而持久化的设计能让consumer重新消费过去的数据。</p>
</blockquote>
<h2 id="4-6-Message-Delivery-Semantics"><a href="#4-6-Message-Delivery-Semantics" class="headerlink" title="4.6 Message Delivery Semantics"></a>4.6 Message Delivery Semantics</h2><p>现在我们理解了一些关于producer和consumer如何工作的知识，现在我们来讨论一下Kafka在producer和consumer之间提供的语义保证（semantic guarantees）。存在多种消息传递保证（message delivery guarantees）：</p>
<ul>
<li>At most once——消息可能丢，但不会重复递送</li>
<li>At least once——消息绝不会丢，但有可能会重复递送</li>
<li>Exactly once——人们真正想要的，每条消息仅递送一次</li>
</ul>
<p>值得注意的是，这被分解为两个问题：</p>
<ol>
<li>消息发送后的持久化保证（durability guarantees）</li>
<li>消费数据时的保证</li>
</ol>
<p>一些系统声称提供exactly once语义，但阅读细则后发现，这些声称都具有误导性（即他们没有考虑consumer或producer会失效的情况，或者存在多个consumer的情况，或者写到磁盘的数据可能丢失的情况）。</p>
<p>Kafka关于这方面的语义是很直截了当的。当发送message时，message存在被提交（committed）这一概念。一旦message被提交，只要相关的broker依然存活（alive），那么它就不会丢失。存活的定义，以及需要处理哪些类型的失效等内容将在下一节描述。现在我们假设broker是完美的，没有数据丢失，然后试着去理解producer和consumer提供的消息传递保证。如果producer发送一条message时遇到网络故障，它无法确定故障是在message提交之前还是之后发生的。这类似于以autogenerated key方式向数据库插入数据时的语义。</p>
<p>上面描述的不是最强级别的递送语义。即使我们不能确定网络故障时发生了什么，但可以通过让producer产生一个有序的主键（primary key）序列，在故障发生时重试发送request，实现幂等性（idempotent，有了key，producer重复发送同一条message，broker会根据key去重）。在一个server很有可能失效的复制系统中（replicated system），要添加幂等特性并非那么简单。<strong>拥有幂等性之后，producer可以超时重试，直到收到message成功提交的响应，如果是这样的话，发送就能够实现exactly once语义。在未来，Kafka想实现这一功能</strong>。</p>
<p>不是所有的使用场景都需要这么强的保证。对于延迟敏感的使用场景，我们允许producer设置持久化级别。Producer可以等待message被提交再返回，也可以配置为完全异步发送，或者只等待leader保存完message就返回，不必等待同步到follower。</p>
<p>现在我们以consumer的视角看递送语义。所有的replica拥有相同的log和offset，consumer控制消费位置。如果consumer不出故障，那么offset可以始终保存在内存里，但如果consumer失效，我们希望相关partition能够被其它consumer接管，并从正确的位置开始消费。我们假设consumer要读取一些message，那么就如何处理message和如何更新offset，有一些不同的情况：</p>
<ol>
<li>Consumer取到数据，在本地保存offset，然后处理数据。这种情况下，consumer有可能在保存offset之后，保存处理结果之前崩溃（注意这里有两次I/O操作），那么之后的consumer会从保存点继续消费，但这个offset之前的若干message还没有进行处理的。这种情况对应at most once语义，即consumer在崩溃时还有数据未处理，之后也不会被处理了。</li>
<li>Consumer取到数据，处理，最后记录offset。这种情况下，consumer有可能在处理完数据之后，保存offset之前崩溃，那么之后的consumer会读取到部分已处理的数据。这种情况对应at least once语义。其实很多情况下每条message都对应一个主键，所以更新操作是幂等的（收到相同的message，覆盖之前的处理结果即可）。</li>
<li>Exactly once语义。这里的难点不在于服务端的消息系统，而是要就各个consumer看到的offet达成一致，即要保存一个完全正确的offset值。<strong>要达成这一目的，最简单的方法就是在保存offset和保存数据处理结果之间引入两阶段提交协议<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4" target="_blank" rel="external">two-phase commit</a></strong>。但consumer可以通过更加简单的方法来处理，把offset作为处理结果的一部分存储在相同的地方。这种方法要更好一些，因为和consumer交互的后端系统可能不支持两阶段提交协议。作为正面例子，Hadoop ETL系统将打入HDFS的数据和对应的offset都存储在HDFS中，因此保证了数据和offset要么同时保存，要么都不保存。我们和许多数据系统一样，要求这种强语义，实现在message没有主键的情况消除重复（deduplication）数据的读取。</li>
</ol>
<blockquote>
<p>这里的两阶段提交的流程：prepare阶段，consumer（作为协调者）将数据丢给处理系统（作为参与者），处理完后结果返回consumer；commit阶段，consumer根据返回的结果发出commit或rollback请求后再次等待，处理系统会真正将处理完的数据写入磁盘，然后返回一个ack，consumer收到响应后根据响应内容决定是否保存offset。使用两阶段提交协议把因为机器故障而造成不一致的概率降到了较低的水平。</p>
</blockquote>
<p>对于producer来说，Kafka默认是at least once递送语义，用户可以禁用失败重试选项来达到at most once递送语义，consumer也可以在处理数据之前保存offset来达到at most once递送语义。实现exactly-once递送语义一般需要下游存储系统的配合，但Kafka使用的方法更加直接。</p>
<h2 id="4-7-Replication"><a href="#4-7-Replication" class="headerlink" title="4.7 Replication"></a>4.7 Replication</h2><p>Kafka根据配置文件中的设置对topic的partition进行复制，你可以给每个topic设置不同的的复制因子（replication factor）。Replication机制能够实现自动故障转移，使得一台server宕机后消息仍然可用。</p>
<p>其它的消息系统也提供了复制功能，但是，就我们的观点而言，这些都是附加的功能，没有得到广泛应用，其最大的缺点在于：<strong>从机是不活跃的（inactive），吞吐量受到很大影响，需要依赖繁琐的手动配置等（例如ActiveMQ，在shared nothing存储模式下，master挂了之后，需要管理员人工介入启动新master）</strong>。Kafka对replication的默认策略是：topic在创建时replication factor为1，即不进行备份。</p>
<p>复制是以partition作为最小单位的。在没有broker失效的情况下，每一个partition在集群中都有唯一的broker作为leader以及0个或多个broker作为follower。一个partition的备份数量（包括leader上的那份）即备份因子。<strong>客户端的读写请求均通过leader</strong>。通常，partition的数量会大于broker，leader的角色将均匀分配到集群的各个broker上。Follower上的log数据和leader上是相同的——相同的offset以及相同的message顺序，当然，在leader末尾的message还未来得及同步到follow时，两者的数据是不一致的。</p>
<p>Follower会像一个普通的conusmer那样从leader消费数据然后保存到本地log文件。<strong>这样做有一个好处是可以利用consumer的batch特性一次拉取多条message，提高吞吐量</strong>。</p>
<p>Kafka和大多数分布式系统一样，需要自动处理失效的情况，因此需要精确定义节点“存活“的含义。对于Kafka，节点的存活包含两个方面：</p>
<ol>
<li>节点必须维持和ZooKeeper的session（通过ZooKeeper的心跳机制）</li>
<li>Follower必须及时同步leader上的新数据，不能“落后”太多</li>
</ol>
<p>我们把满足以上两点的节点称之为“同步节点”（in sync），以此来把它和“存活”“失效”等概念区分开来。Leader会跟踪记录所有同步节点。如果一个follower挂了或卡住了，或数据同步落后太多，那么leader会把它从同步节点列表中移除，移除条件由配置项<em>replica.lag.time.max.ms</em>来确定。</p>
<p>在分布式系统领域，我们只关注fail/recover类型的失效，即节点突然停止工作并在一段时间后恢复正常。Kafka不会处理拜占庭式的失效，即节点有可能产生任意或恶意的response。</p>
<p>一条message被所有的同步节点同步后（其实是对message所在partition的同步），就可以认为它已经提交了（committed）。只有提交了的message才能交付给consumer端。这意味着consumer不必担心由于leader失效可能导致的message丢失。另一方面，为了在延迟和持久化之间进行权衡，producer可以选择是否等待message提交。Producer选择哪一种策略是根据配置项<em>request.required.acks</em>来确定的。</p>
<p>Kafka保证只要有至少一个同步节点存活，已提交的message就不会丢失。</p>
<blockquote>
<p>这里只是说提交成功的message不会丢失，并没有说producer发送成功的message不会丢失</p>
</blockquote>
<p>Kafka在短时间的失效转移时间内仍然可用，但网络分区的情况下不保证始终可用，<a href="https://aphyr.com/posts/293-jepsen-kafka" target="_blank" rel="external">Kafka选择了CA而牺牲了P，但一旦出现网络分区，还是会牺牲可用性</a>。</p>
<blockquote>
<p>个人认为，当发生分区时，CA会最终转换成CP或AP。例如，当出现网络分区，producer会因为发送失败返回错误，这就不是100%可用了。因此，分布式系统必须要具备分区容忍性。</p>
</blockquote>
<h3 id="Replicated-Logs-Quorums-ISRs-and-State-Machines"><a href="#Replicated-Logs-Quorums-ISRs-and-State-Machines" class="headerlink" title="Replicated Logs: Quorums, ISRs, and State Machines"></a>Replicated Logs: Quorums, ISRs, and State Machines</h3><p>Kafka partition的核心是复制log（replicated log）。在分布式数据系统中，log复制是最为重要的概念之一，实现方法也非常多。Log复制可以被应用于很多分布式系统中，作为复制状态机概念的一部分。</p>
<p>一个要复制的log可以看做是一个处理模型，目的是通过协商使一个序列内的元素（log entry）顺序达成一致，即log entry按顺序0,1,2…排列。实现这一目的的方法很多，最为简单且快速的方法是：系统中有一个leader，由它来决定log entry的排列顺序。只要leader存活，所有follower只需要按照这个顺序将log entry拷贝过来就可以了，即log entry的顺序在所有节点上达成了一致。</p>
<p>如果leader不挂，我们当然可以不需要follower。但如果leader挂了，我们需要从follower中选出一个新的leader。但follower本身可能会落后leader很多，也可能会挂，所以我们要确保选出一个最新（up-to-date）的follower。对于log复制算法，Kafka最基本的保证是：当Kafka告知client一个message已经被提交，并且leader挂了，新选出的leader必须存有这条message。这里需要进行权衡：如果leader需要等待越多的follower返回message提交成功，那么就有更多的follower有资格成为leader，但延迟会提高。</p>
<p>如果选择一个follower的数量，这些follower和leader上log的内容相同或是leader的子集，那么这个数就叫做法定人数(<a href="https://en.wikipedia.org/wiki/Quorum_(distributed_computing" target="_blank" rel="external">Quorum</a>))。</p>
<p>一种权衡一致性和延迟这两方面的算法是：对于确定决议和leader选举，采用大多数选票（majority vote）的方法，少数服从多数。<strong>Kafka没有这样做，但这种方法提供了一种如何进行权衡的思路</strong>。假设我们有2f+1个备份，那么当满足如下条件时，leader宕机后新选举出的leader能够保证拥有所有已经被提交的数据：</p>
<ol>
<li>leader宣布数据已经提交之前，f+1个备份已经成功接收了这份数据</li>
<li>选举新leader时，从至少为f+1（和上面的f+1不是同一个集合）个备份中选出拥有最完整log的备份成为新leader</li>
<li>不超过f个备份节点失效</li>
</ol>
<p>因为在不多于f个备份节点失效的情况下，任何f+1个备份节点中一定存在至少一个备份节点包含所有已经提交的数据（可以考虑极端情况下，f+1个成功接收的备份节点中有f个失效，仍然会有最有一个节点保存有完整的数据）。这个节点上的log是最完整的，因此它会被选为新leader。这些算法还有很多细节需要处理（例如需要精确定义如何让log更加完整，确保在leader失效或服务器数量发生变化时log的一致性），但我们现在忽略这些细节。</p>
<p>这种获得大多数选票获胜的算法有一个很好的特性：延迟只依赖于那些最快的服务器。也就是说，如果备份因子为3，那么延迟由最快的slave决定，而不是最慢的。</p>
<p>这类算法有许多，包括ZooKeeper的Zab，Raft，Viewstamped Replication。我们认为和Kafka最接近的算法是微软的PacificA。</p>
<p>Majority vote算法的缺点是它无法处理多个节点失效导致没有可以选择的候选leader这种情况。容忍1个节点失效需要3份拷贝，容忍2个节点失效需要5份拷贝。在我们的经验中，单纯的增加冗余来对应单点失效是不够的，但5倍次数的写，5倍存储空间和1/5的吞吐量在处理大数据时又是不太实际的。</p>
<blockquote>
<p>这里的大概意思是：如果采用majority vote策略，增加拷贝还是不能容忍大多数节点失效这种情况；而要能容忍大多数节点失效，就需要每次write都同步复制到其它从节点，而这种同步复制的效率又是不能接受的。</p>
</blockquote>
<p>这可能就是为什么quorum算法常常出现在存储元数据的系统中，例如ZooKeeper，而很少出现在原始数据的存储中。例如HDFS的namenode的高可用性就是以<a href="http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1/" target="_blank" rel="external">majority vote</a>作为基础，但它并没有用在datanode中。</p>
<p>Kafka选择了采用了稍微不同的方法来选择quorum集合。不同于majority vote，Kakfa动态地维护一个叫做in-sync replicas（ISR）的集合，集合内的节点去追赶复制leader。只有这个集合中的成员才有资格选举为leader。<strong>只有所有ISR都接收了数据，才认为数据被提交成功</strong>。ISR集合保存在ZooKeeper中，动态变化，所以当需要选举leader时，ISR中的成员是有资格成为leader的。对于Kafka的使用模型来说，这是很重要的一点，Kafka的特点是有多个partition并且leader在集群中的分布是非常平衡的。通过ISR模型和f+1个备份，Kafka可以在f个节点失效的情况下不丢失已提交的数据。</p>
<blockquote>
<p>从上面的描述可以看出，Kafka其实也采用了quorum-based技术，只是确定quorum集合的方法和常规的majority vote有所不同，majority vote把备份的大多数作为法定人数，而ISR技术确定法定人数是根据ISR数量确定的。</p>
</blockquote>
<p>对于我们关注的许多场景，这样一种权衡的做法是合理的。实际上，为了容忍f个节点失效，majority vote算法和ISR策略都需要等待f+1个备份节点返回确认才算提交成功（例如，如果需要容忍1个节点失效，majority vote算法需要3个备份节点和至少1个备份节点返回确认，而ISR算法只需要2个备份节点和至少1个备份节点的返回确认）。<strong>Majority vote的优势在于提交操作不依赖于最慢的那些服务器</strong>。但是，我们认为让用户选择在提交操作时是否阻塞更加合理，并且用更小的备份因子换取更大的吞吐量和更小的磁盘空间是很划算的。</p>
<p>Kafka另一个独有设计是，它不要求失效的节点在恢复后拥有完好无损的全量数据。这有别于一般的复制算法稳定存储（stable storage）的思想，这些复制算法不允许故障恢复后出现丢失数据或是不一致的情况。但这一思想有两个问题：  </p>
<ol>
<li>在持久化存储系统中，最常见的就是磁盘错误，而磁盘错误经常时数据无法保持完整无缺  </li>
<li>即使1不是问题，我们也不想为了保证一致性而每次写操作都调用fsync，这会使性能降低2到3个数量级<br>Kafka的协议允许备份节点恢复工作后重新加入ISR，加入之前必须全部重同步所有的数据。<blockquote>
<p>这里的意思应该是，为了效率，同步到follower的数据不会立刻刷到磁盘，而是保存在内存中，但这样就会存在宕机丢失数据的风险，Kafka通过失效重启后需要跟leader同步，恢复未刷入磁盘的数据。</p>
</blockquote>
</li>
</ol>
<h3 id="Unclean-leader-election-What-if-they-all-die"><a href="#Unclean-leader-election-What-if-they-all-die" class="headerlink" title="Unclean leader election: What if they all die?"></a>Unclean leader election: What if they all die?</h3><p>Kafka在数据持久化方面的的保证是，至少存在一个replica在ISR中。如果一个partition的所有replica都挂了，那么这样的保证也就不存在了。</p>
<p>但是，一个实际的系统需要在所有replica都挂了之后做某些合理的事情。如果这样的事情不幸发生，考虑之后会发生什么是很重要的。具体的实现可以有两种：</p>
<ol>
<li>不允许unclean leader election，等待一个拥有全量数据并且属于ISR的replica恢复并将它选为leader</li>
<li>允许unclean leader election，选择第一个恢复的replica（不一定属于ISR）作为leader</li>
</ol>
<p>这是一个在可用性和一致性之间的权衡。如果我们等待ISR中的replica，那么只要没有replica恢复，服务将始终不可用。如果这些replica已经损坏或数据已经丢失，那么服务将永久失效。另一方面，如果一个不属于ISR的非同步（non-in-sync）replica恢复正常，然后我们允许它成为leader，那么它的log数据将作为信息源（source of truth），即使它不保证拥有每一条已提交数据。<strong>在当前的版本中我们选择第二种策略，即当ISR的所有节点都宕机时，更倾向于数据的不一致</strong>。未来，我们会让这些策略变成可配置的，以此来适应不同的场景。</p>
<p>这一窘境不只是在Kafka中出现，它也出现在以quorum为基础的模式中。例如majority voting模式，如果大多数服务器失效，那么数据不保证100%完整，一致性就有可能被破坏。</p>
<h3 id="Availability-and-Durability-Guarantees"><a href="#Availability-and-Durability-Guarantees" class="headerlink" title="Availability and Durability Guarantees"></a>Availability and Durability Guarantees</h3><p>当向Kafka发数据时，producer可以选择是否等待0个、1个或所有（-1）replica的回应。注意，等待所有replica的回应只是等待IRS收到数据而不是所有的replica都收到数据。默认的，当<em>request.required.acks</em>=-1时，只要当前ISR集合收到数据，就会返回响应。例如，如果一个topic的备份因子是2，一个replica挂了（ISR内节点的数量为1），那么<em>request.required.acks</em>=-1时写操作能成功。但是，如果剩下的replica也挂了，那么发送的数据将会丢失。虽然这确保了系统的可用性，但对于更加关注持久性的用户来说，这种策略是不合适的。因此，我们提供了关于可用性和持久性两方面的topic级别的两个配置。</p>
<ol>
<li>禁止unclean leader election（配置项<em>unclean.leader.election.enable</em>）。如果所有的replica不可用，那么这个partition将会变得不可用，直到ISR中的成员恢复正常。这一策略更倾向于数据的持久性，而不是可用性。</li>
<li>指定一个最小的ISR规模（配置项<em>min.insync.replicas</em>）。写入的数据只有被这个数量以上的replica接收才被认为是提交成功。这能够防止写入单个replica而可能造成的数据丢失。这一设置只有在producer配置了<em>required.acks</em>=-1时生效，它保证了一条message至少会有这么多数量的ISR成员回应ack。这一设置权衡了一致性和可用性。<em>min.insync.replicas</em>设置的越大，保证能够写入的replica越多，丢失数据的风险也越小，因此具有更好的一致性。但是，它降低了可用性。因为写入数据时，如果ISR数量小于<em>min.insync.replicas</em>，那么partition将变得不可用。</li>
</ol>
<h3 id="Replica-Management"><a href="#Replica-Management" class="headerlink" title="Replica Management"></a>Replica Management</h3><p>上面讨论的关于replicated log的内容只针对单个log，即一个topic的一个partition。但一个Kafka集群可能要管理成百上千的partition，我们会采用轮询（round-robin）的方式均匀分布一个集群中的partition，避免许多都partition都堆积在小部分节点上。同样，leader的分配也做到均匀，使得每个节点只成为部分partition的leader。</p>
<p>Leader选举时会有一段时间的不可用，对这段时间的优化也是很重要的。最原始的实现是，当leader挂掉后，拥有此partition的所有节点都要开始选举过程。<strong>作为替代，我们选择一台broker作为controller。Controller负责检测broker是否失效，当检测到失效后，它会将受影响的partition（这些partition的leadership在这台失效broker上）的leadership转给其它broker。这样做的结果是，Kafka能够处理大量的leadership转移的需求，对于partition很多的情况，选举过程也能做到更轻、更快</strong>。如果Controller挂了，会选一台存活的broker作为新Controller。</p>

      
    </div>
    
    <div>
      
        
      
    </div>

    <div>
      
        
      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/04/17/hello/" rel="next" title="First Blood">
                <i class="fa fa-chevron-left"></i> First Blood
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/04/22/ActiveMQ/" rel="prev" title="ActiveMQ">
                ActiveMQ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpeg"
               alt="Ouyang Liduo" />
          <p class="site-author-name" itemprop="name">Ouyang Liduo</p>
          <p class="site-description motion-element" itemprop="description">每个认真生活的人，都值得被认真对待。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Design"><span class="nav-number">1.</span> <span class="nav-text">4 Design</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Motivation"><span class="nav-number">1.1.</span> <span class="nav-text">4.1 Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Persistence"><span class="nav-number">1.2.</span> <span class="nav-text">4.2 Persistence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Don’t-fear-the-filesystem"><span class="nav-number">1.2.1.</span> <span class="nav-text">Don’t fear the filesystem!</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Constant-Time-Suffices"><span class="nav-number">1.2.2.</span> <span class="nav-text">Constant Time Suffices</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-The-Producer"><span class="nav-number">1.3.</span> <span class="nav-text">4.4 The Producer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Load-balancing"><span class="nav-number">1.3.1.</span> <span class="nav-text">Load balancing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asynchronous-send"><span class="nav-number">1.3.2.</span> <span class="nav-text">Asynchronous send</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-The-Consumer"><span class="nav-number">1.4.</span> <span class="nav-text">4.5 The Consumer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Push-vs-pull"><span class="nav-number">1.4.1.</span> <span class="nav-text">Push vs. pull</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumer-Position"><span class="nav-number">1.4.2.</span> <span class="nav-text">Consumer Position</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Offline-Data-Load"><span class="nav-number">1.4.3.</span> <span class="nav-text">Offline Data Load</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-Message-Delivery-Semantics"><span class="nav-number">1.5.</span> <span class="nav-text">4.6 Message Delivery Semantics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-Replication"><span class="nav-number">1.6.</span> <span class="nav-text">4.7 Replication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Replicated-Logs-Quorums-ISRs-and-State-Machines"><span class="nav-number">1.6.1.</span> <span class="nav-text">Replicated Logs: Quorums, ISRs, and State Machines</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unclean-leader-election-What-if-they-all-die"><span class="nav-number">1.6.2.</span> <span class="nav-text">Unclean leader election: What if they all die?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Availability-and-Durability-Guarantees"><span class="nav-number">1.6.3.</span> <span class="nav-text">Availability and Durability Guarantees</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Replica-Management"><span class="nav-number">1.6.4.</span> <span class="nav-text">Replica Management</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ouyang Liduo</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  

  

  

</body>
</html>
