<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka官方文档翻译">
<meta property="og:url" content="http://yoursite.com/2016/04/18/kafka官方文档翻译/index.html">
<meta property="og:site_name" content="Stay hungry. Stay foolish.">
<meta property="og:description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">
<meta property="og:image" content="http://yoursite.com/images/kafka/top.png">
<meta property="og:image" content="http://yoursite.com/images/kafka/topic.png">
<meta property="og:updated_time" content="2016-07-27T01:00:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kafka官方文档翻译">
<meta name="twitter:description" content="本篇为Kafka官方文档翻译，加入了自己的理解。">
<meta name="twitter:image" content="http://yoursite.com/images/kafka/top.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>







  <title> Kafka官方文档翻译 | Stay hungry. Stay foolish. </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Stay hungry. Stay foolish.</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/04/18/kafka官方文档翻译/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Ouyang Liduo">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpeg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Stay hungry. Stay foolish.">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Stay hungry. Stay foolish." src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Kafka官方文档翻译
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本篇为Kafka官方文档翻译，加入了自己的理解。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Kafka是一个能够保存数据的服务，具有分布式（distributed）、数据分区存储（partitioned）、主从复制（replicated）等特点。它具有消息队列的功能，但自身又有一些独特的设计。</p>
<p>那么这意味着什么呢？</p>
<p>首先让我们回顾一下消息系统中的术语：</p>
<ul>
<li>Kafka将接收到的消息分类，用<em>topic</em>区分，每一条消息都属于一个topic</li>
<li>我们把能够往Kafka某个topic上发送消息的进程称之为<em>producer</em></li>
<li>我们把订阅了topic并能够处理收到的消息的进程称之为<em>consumer</em></li>
<li>Kafka作为一个集群运行，集群内由一台或多台服务器组成，每台服务器称作一个<em>broker</em></li>
</ul>
<p>所以，站在更高的角度来看，producer通过网络发送数据到Kafka集群，然后这些数据又被consumer消费，如下图所示：</p>
<p><img src="/images/kafka/top.png" alt=""></p>
<p>客户端（producer、consumer）和服务端（broker）的通信直接使用了TCP来完成，具体协议<a href="https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol" target="_blank" rel="external">在这里</a>。我们提供了Java客户端，其它语言的客户端可以<a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank" rel="external">在这里</a>找到。</p>
<h3 id="Topics-and-Logs"><a href="#Topics-and-Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h3><p>首先让我们深入理解一个Kafka提供的高层次抽象（high-level abstraction）——topic。</p>
<p>一个topic就是一类数据（消息）的名称，对于每一个topic，Kafka将一个topic分成多个partition，数据以log文件的形式保存在集群中，如下图所示：</p>
<p><img src="/images/kafka/topic.png" alt=""></p>
<p>每一个partition是一个提交日志（commit log），内部存放的message有序且顺序不可变，新的message追加到文件末尾。在partition中，每一条message都分配了一个序列ID，称为<em>offset</em>，在一个partition中，offset唯一标识每一条message。</p>
<p>Kafka集群保存所有由producer发送过来的数据，不论这些数据有没有被消费，保存时间可以设置。例如，如果配置的保存时间是2天，那么message发送成功之后的2天里，这条message都是可被消费的，之后会被删除以释放空间。<strong>Kafka的性能与保存的数据量无关，所以保存大规模数据不是问题。</strong></p>
<p>实际上，对于consumer来说，它唯一需要保存的元数据就是当前在log文件中的消费位置（当前消费到哪条message了），即offset。这个offset是由consumer来控制的：通常consumer会在读取message之后递增offset，但offset是由consumer控制，因此它可以以任何顺序消费message。例如，consumer可以把offset回滚到一个更小的值，然后重新消费旧的数据。</p>
<p>这些特征都说明consumer的操作开销是很小的——它们来去自如，不会对集群或其它consumer造成太多影响。例如，你可以使用我们的工具去tail任何topic的内容而不会改变现有consumer的消费位置。</p>
<p>提出partition的概念有多个目的。首先，它是的数据规模可以自由扩展，不再依赖于单台服务器的存储量。每一个partition的大小不能超过存储它的机器的容量（因为一个partition只能保存在一台机器中），但一个topic可以有很多的partition，因此一个Kafka集群能够保存任意规模的数据（通过增加机器数并增加partition数来实现扩容）。第二，每一个partition都是一个并行单元（unit of parallelism），实现了并行消费，之后会详细介绍。</p>
<h3 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h3><p>Log的partition被分布在一个Kafka集群的多台服务器上，每台服务器保存所有partition的一部分。每一个partition能够复制到其它服务器，也就是多台服务器存储同一个partition的数据，通过这种方式达到容错，复制多少分是可以设置的。</p>
<p>对于一个partition，在保存它的所有服务器中有一台作为<em>leader</em>，零台或多台作为<em>follower</em>。Leader处理和这个partition相关的所有读写请求，而follower只是被动的复制leader的数据。如果leader挂了，follower中的一个将会自动提升为新的leader。对于一台服务器，所有保存在它上面的partition中，部分partition的leader是这台服务器，另外一部分partition的leader是其它服务器，这使得leader能够均匀的分布在集群中，保证了负载均衡。</p>
<blockquote>
<p>Kafka只有开启了replica机制才会有follower，当partition没有备份时，唯一存储这个partition的服务器就是leader。</p>
</blockquote>
<h3 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h3><p>Producer根据选择发布数据到指定的topic，它能够选择将数据发送到topic的哪个partition。可以通过轮询算法将数据均匀的发到各个partition，或使用一些语义分区函数（semantic partition function）根据语义将不同message发到不同partition（也就是基于key的message）。</p>
<h3 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h3><p>消息系统通常有两种模型：<a href="https://en.wikipedia.org/wiki/Message_queue" target="_blank" rel="external">队列模型</a>和<a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern" target="_blank" rel="external">订阅-发布模型</a>。在队列模型中，多个consumer从同一个server上读数据，一条message被一个consumer取走；在订阅-发布模式中，一条message能够广播到所有的consumer上。Kafka提供了一个独立consumer的抽象来包含这些特点——<em>consumer group</em>。</p>
<p>每个consumer都带有一个group，表示自己属于哪个group。<strong>一个topic内的一条message只会被订阅了这个topic的group内的一个consumer实例消费</strong>，consumer实例可以是相互独立的进程或机器。</p>
<p><strong>如果所有consumer实例都属于同一个group，那么这相当于一个队列模型，message均匀发送到各个consumer。</strong></p>
<p><strong>如果所有consumer实例都在不同的group，那么这相当于一个订阅-发布模式，一条message会被所有consumer接收。</strong></p>
<p>更通常的情况是，一个topic有少量的group在消费它，每个group相当于一个逻辑上的订阅者，每个group由许多的consumer组成，保证了伸缩性和容错性。对Kafka来说，订阅者变成了一组consumer而不是一个单一的consumer。</p>
<p>相比于传统的消息系统，Kafka具有很强的顺序保证。</p>
<p>传统的队列模型将message有序存放在server上，当有多个consumer消费这个queue时，server会按照存储的顺序发送给各个consumer。然而，即使数据是有序存放的，但发送是异步的，所以接收顺序并不一定是发送顺序。这意味着在并行消费的情况下，消息的顺序无法保证。为了解决这个问题，消息系统通常会提出exclusive consumer的概念，一个queue只允许一个进程消费，但这样意味着失去了并行性。</p>
<p>Kafka能够很好的处理这个问题。通过partition，Kafka能够保证message的消费顺序和多个consumer同时消费时的负载均衡。这是通过将一个partition分配给一个consumer来实现的。通过这种方法，我们能够确定一个partition只能被同一group里的一个consumer消费，不能被同一group里的两个consumer同时消费，那么这个唯一consumer消费到的message就是有序的。注意，一个group内的consumer数量不能大于它消费的topic内partition数量。</p>
<p>Kafka只保证partition内的message能够被顺序消费，而不同partition之间的message消费顺序是不确定的，结合按key发送数据到特定partition这一特性，Kafka能够满足大多数应用程序的需求。然而，如果你想要所有message的完全有序，那么可以只给这个topic设置一个partition，这意味着一个group内只能有一个consumer消费它。</p>
<h3 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h3><p>站在一个更高的角度，Kafka具有如下保证：</p>
<ol>
<li>一条message发送到一个topic的某个partition上时，它会按照发送顺序被追加到server的log文件中。也就是说，如果M1和M2被同一个producer发送，且M1先于M2被发送，那么M1具有更小的offset值。</li>
<li>一个consumer实例消费的顺序是log文件保存数据保存的顺序。</li>
<li>如果一个topic的备份因子为N，我们能够允许N-1台服务器宕机而不丢失已经提交了的数据。</li>
</ol>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>我们设计Kafka的目的是为了提供一个能够处理实时数据的统一平台。为了实现这一目标，我们需要考虑很多使用场景。</p>
<p>它必须拥有高吞吐量（high-throughput），以支持规模庞大的数据流，例如实时日志的聚合（aggregation）。</p>
<p>它需要优雅地处理海量数据的保存（backlogs），以便支持周期性的从离线系统中加载数据。</p>
<p>它还要保证低延迟传输，满足传统消息系统的使用场景。</p>
<p>我们想让系统支持分区（partitioned）、分布式（distributed）等特性。</p>
<p>最后，当数据进来后，我们需要在机器出现故障时，系统具有容错性。</p>
<p>我们想将这些使用场景分别对应独立的设计单元，使得它更像是一个数据库而不是消息系统。我们将会在下面的章节中概括一些设计元素。</p>
<h2 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h2><h3 id="Don’t-fear-the-filesystem"><a href="#Don’t-fear-the-filesystem" class="headerlink" title="Don’t fear the filesystem!"></a>Don’t fear the filesystem!</h3><p>Kafka非常依赖文件系统来存储（storing）或缓存（caching）数据。普遍观点认为磁盘速率很慢，这使得人们对持久化设备能够提供卓越的性能这一观点持怀疑态度。<strong>实际上，磁盘的数据有可能比我们想象的要慢，也可能比我们想象的要快，这取决于如何使用它，一个合理的磁盘结构设计通常比网络速率要快。</strong></p>
<p>磁盘性能的关键点在于磁盘驱动器的吞吐量，在过去的十多年里，这一指标不再由磁盘寻道时间决定。性能测试结果表明，在<a href="https://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank" rel="external">JBOD</a>结构的六块7200rpm SATA盘上，顺序写性能大约是600MB/sec，但是随机写只有大约100k/sec，相差6000倍。由于顺序读写是最可能被预测的（most predictable）一种使用模式（程序需要读取第N个字节，我能够预测它下次要读取第N+1个字节），因此操作系统对它有很大的优化空间。现代操作系统提供了预读（read-ahead，预先从多个磁盘块读取数据）和后写（write-behind，合并小的逻辑写操作为一个大的物理写操作）机制。下面讨论的内容可以在<a href="http://queue.acm.org/detail.cfm?id=1563874" target="_blank" rel="external">ACM Queue article</a>找到，这篇文章说，在某些情况下，顺序的磁盘操作要比随机的内存操作快。</p>
<p>为了弥补磁盘和内存之间的性能差距，现在操作系统尽可能的利用主存来缓存磁盘数据。一个现代操作系统很乐意将所有的空闲内存都用作磁盘缓存，即使在内存回收（reclaimed）时会带来一定的性能损耗。所有的磁盘读写都要经过内存这一层缓存，除非使用直写I/O（<a href="https://www.ibm.com/developerworks/cn/linux/l-cn-directio/" target="_blank" rel="external">direct I/O</a>），所以，即使一个进程在进程内缓存（in-process cache）中保存了数据，操作系统也会将数据冗余存储在页缓存（pagecache）中，实现有效的冗余存储。</p>
<p>我们的程序运行在JVM之上，任何人在任何时候使用了JVM内存，都会有以下两个结果：</p>
<ol>
<li>对象的内存开销很大，经常是对象本身大小的两倍甚至更多</li>
<li>当堆中的数据增加时，垃圾回收变得非常繁琐且缓慢</li>
</ol>
<p>由于这些原因，我们使用文件系统并依赖页缓存机制比把数据存储在用户空间的内存或其它结构中要更好一些。这样我们就拥有了两倍的缓存，并存储两份数据。在32GB内存的系统中，缓存容量可以达到28-30GB。并且，即使服务重启了，缓存还是会存储热数据，而进程内缓存的数据需要重新建立（10G的缓存大约需要10分钟），或者完全根据冷数据建立（这将严重影系统启动时性能）。这也极大简化了代码中关于维持缓存和文件系统一致性的相关逻辑，因为这些工作都交给了操作系统，这比程序自己去实现要更加高效正确。如果磁盘以线性读为主，那么每次从磁盘读到的数据缓存起来对效率的提升是非常有帮助的。</p>
<p>这些特性使得我们的设计非常简单：相比于把所有数据都存储在用户空间内存中，等到空间耗尽时再一次性刷入磁盘，我们恰好相反，所有的数据立即写入文件系统上的持久化日志（persistent log）中但并没有刷入磁盘，实际上数据只是传输到了内核的页缓存中。</p>
<p>这种围绕页缓存为中心的设计（pagecache-centric design）在<a href="http://varnish-cache.org/docs/trunk/phk/notes.html" target="_blank" rel="external">这里</a>有描述。</p>
<h3 id="Constant-Time-Suffices"><a href="#Constant-Time-Suffices" class="headerlink" title="Constant Time Suffices"></a>Constant Time Suffices</h3><p>对于数据要持久化存储的消息系统，每个consumer都会分配一个queue，这个queue会关联一个BTree或其它能加速随机存取的数据结构。B树是最通用的数据结构，能在消息系统中支持事务、非事务性场景，但它带来的开销也很大。虽然B数的时间复杂度是O(log N)，通常认为O(log N)本质上等价于常数时间，但对于磁盘操作来说并非如此。磁盘寻道时间平均在10ms左右，每一个磁盘在一个时刻只能进行一个寻道操作，所以并行性受到了限制。所以少量的磁盘寻道也能造成非常大的开销。<strong>因为存储系统把快速的缓存操作和慢速的磁盘操作结合起来了，所以树结构的性能会随着数据量的增长呈超线性（superlinear）关系——数据量增长一倍，速度下降大于一倍。</strong></p>
<p>直觉上，一个持久化队列可以构建在简单的文件读或追加上。这种结构有一个优势就是所有操作都是O(1)，读不会阻塞写，反之也是。这是一个很明显的性能优势，因为性能完全和数据量解耦了，因此一台server能够充分利用廉价、低速的大容量（大于1TB）SATA硬盘。虽然寻道操作性能低下，但对于大块数据的读写，这一时间比例会缩小，并且我们可以获得1/3的价格和3倍大的容量。</p>
<p>拥有一个很大的磁盘空间，并且性能不会随着空间增大而降低，这使得我们可以提供一些其它消息系统没有的特性。例如，不同于message消费完后就立刻被删除，我们能够将message保留相当长一段时间（例如一周），这给consumer带来了很大的灵活性，我们将会在下面描述。</p>
<blockquote>
<p>上面都是在说明Kafka的数据存储结构，为什么使用了简单的文件读写而没有使用BTree（数据库）。一是空间性能比，二是持久化存储，可见文件存储也是Kafka的一大特性。在文件系统中，B树用来找到inode对应的块。</p>
</blockquote>
<h2 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h2><p>我们在提高效率方面投入了大量精力。Kafka的主要使用场景之一是处理web相关活跃数据，这个量是非常大的，一个PV请求可能产生几十个条写数据。并且我们假设每条message会被至少一个consumer消费，因此我们致力于让消费更加轻量。</p>
<p>根据经验我们发现，对于multi-tenancy architecture系统（一个实例被多个客户共享，每个客户称作一个tenant），效率是关键。如果服务端因为客户端使用模式的改变而造成性能瓶颈，那么就会产生问题。因此只有确保消费过程足够快，服务端才有可能不被客户端程序压垮（客户端先过载）。这对于一个中心化服务（centralized service）架构的系统来说是非常重要的，这样的系统要服务几十到上百个客户端，并且客户端的使用模式（usage patterns）几乎每天都会变化。</p>
<p>我们在前一节讨论了磁盘效率，一旦不好的磁盘访问模式被消除，又会有两个问题导致低效：</p>
<ol>
<li>过多的小I/O操作</li>
<li>大量的字节拷贝</li>
</ol>
<p>小I/O问题在客户端和服务端都会出现（服务端的持久化操作）。</p>
<p>为了避免这类问题，我们的协议围绕着message set的概念，将message聚集起来。这使得一次request可以返回多个message，减轻了网络传输带来的开销。Server端一次性追加所有的message，consumer也可以一次获取一批顺序的message。</p>
<p>这个简单的优化带来了巨大的效果，Batching造成更大的网络包，更大的磁盘顺序操作，相邻的内存块等等。所有这些使得Kafka能够将随机写产生的突发数据流（bursty stream）转变为顺序写，然后传给consumer。</p>
<blockquote>
<p>第一个问题搞定</p>
</blockquote>
<p>另一个低效来自于字节拷贝。当message少时这不是问题，但负载过高时效果明显。为了避免这个问题，我们采用了标准的二进制消息格式，这种格式的消息在producer、broker、consumer之间传递，因此传输数据块可以不用修改格式。</p>
<p>Broker存储message的log只是目录中的一些文件，每一个文件被message序列填充，producer、consumer使用的格式和保存在文件中的格式是相同的。维持这种通用格式是的我们能够优化一个最重要的操作：通过网络传输大块持久化数据。在将数据从页缓存传输到socket的操作上，现代UNIX操作系统提供了高度优化的代码，<strong>在Linux系统中是通过<a href="https://www.ibm.com/developerworks/library/j-zerocopy/" target="_blank" rel="external">sendfile</a>系统调用来实现的。</strong></p>
<p>为了理解sendfile，首先需要了解数据从文件到socket的正常路径：</p>
<ol>
<li>操作系统从磁盘读数据到内核页缓存（DMA拷贝）</li>
<li>应用程序从内核读数据到用户层buffer（CPU拷贝）</li>
<li>应用程序写数据到内核的socket buffer中（CPU拷贝）</li>
<li>操作系统把socket buffer中的数据拷贝到NIC buffer中，最后发送到网络中（DMA拷贝）</li>
</ol>
<p>这样的操作显然效率很低，共有四次数据拷贝和两次系统调用。如果使用sendfile，那么操作系统会将页缓存中的数据直接发送，所以优化之后，只需要将数据拷贝到NIC buffer中就可以了。</p>
<blockquote>
<p>优化后的传输路径变为：文件 -&gt; 页缓存 -&gt; socket buffer -&gt; NIC buffer</p>
</blockquote>
<p>我们希望一个topic能够被多个consumer消费，使用上面描述的zero-copy优化（sendfile是zero-copy技术在Linux下的一种实现），数据拷贝到页缓存之后，可以被多个consumer重复利用，避免了每次read都要从内核空间拷贝出来。<strong>这使得数据被消费的速率只受限于网络（而不是I/O）。</strong></p>
<blockquote>
<p>第二个问题搞定</p>
</blockquote>
<p>页缓存技术和sendfile结合后，一个Kafka集群在被多个consumer消费时，不会看到频繁地读磁盘操作，所有需要消费的数据都放在了内核页缓存里。</p>
<p>了解更多sendfile和zero-copy的背景，<a href="https://www.ibm.com/developerworks/linux/library/j-zerocopy/" target="_blank" rel="external">看这里</a>。</p>
<h3 id="End-to-end-Batch-Compression"><a href="#End-to-end-Batch-Compression" class="headerlink" title="End-to-end Batch Compression"></a>End-to-end Batch Compression</h3><p>许多时候，性能的瓶颈不是在CPU和磁盘，而是在网络带宽，尤其是在广域网的数据中心之间传输局时。用户可以自行压缩每一条message而不借助于Kafka。但是相同类型的message还是会产生冗余，造成压缩率低下（例如JSON的字段名，每条message都会重复存储）。<strong>高效的压缩应该是将多个message作为一个整体压缩，而不是单条message独立压缩。</strong></p>
<p>Kafka支持对message set的压缩，一批message整体被压缩然后发送到server，然后以相同格式保存在log中，最后由consumer解压缩。</p>
<p>Kafka支持GZIP和Snappy压缩协议，更多细节<a href="https://cwiki.apache.org/confluence/display/KAFKA/Compression" target="_blank" rel="external">看这里</a>。</p>
<h2 id="The-Producer"><a href="#The-Producer" class="headerlink" title="The Producer"></a>The Producer</h2><h3 id="Load-balancing"><a href="#Load-balancing" class="headerlink" title="Load balancing"></a>Load balancing</h3><p>Producer将数据直接发送给partition对应的leader，中间没有其它层。为了帮助producer完成发送，Kafka节点能够返回元数据给producer，内容包括哪些broker存活，topic的各个partition的leader是谁，以便producer将数据发送到正确的节点上。</p>
<p>Client端决定将数据发送到哪些partition。这可以通过随机算法或根据语义分区函数（semantic partitioning function）来定位partition。我们开放了语义分区相关的接口，让用户根据key映射partition（分区函数可以通过配置修改）。例如，key是用户ID，那么同一个用户的所有数据会打到同一个partition上，这反过来允许consumer对它们的消费做局部性假设（locality assumptions）。这样的partition分配策略允许consumer对局部敏感（locality-sensitive）的数据进行处理。</p>
<h3 id="Asynchronous-send"><a href="#Asynchronous-send" class="headerlink" title="Asynchronous send"></a>Asynchronous send</h3><p>Batching是提升性能的关键点。Kafka producer可以把数据聚集在内存中，然后一次性在一个request中发出去。可以设置最大聚集量和最大等待时间（例如64k或10ms）。Producer积累的数据越多，broker就具有更少的I/O次数，但一次I/O传输的数据量会更多。这一配置提供了一个在增大延迟和提高吞吐量之间的权衡。</p>
<h2 id="The-Consumer"><a href="#The-Consumer" class="headerlink" title="The Consumer"></a>The Consumer</h2><p>Consumer通过发送fetch请求来获取数据。Consumer在请求中指定log的offset，然后就能够收到从这个offset开始的数据了。Consumer对offset拥有控制权，因此可以通过操控offset来重复消费。</p>
<h3 id="Push-vs-pull"><a href="#Push-vs-pull" class="headerlink" title="Push vs. pull"></a>Push vs. pull</h3><p>我们需要考虑的一个首要问题是consumer是从broker拉（pull）数据还是broker推（push）数据到consumer。在这方面，Kafka遵从传统的设计，和大多数消息系统一样，producer push数据，consumer pull数据。但一些日志系统，例如Scribe和Apache Flume，是讲数据push到下游的。不论是push还是pull，都有各自的优缺点。对于基于push的系统，当consumer类型不同时，broker难以对不同consumer传输速率进行控制。Consumer的目标是尽可能快的消费到数据，但在基于push的系统中，当消费速率落后于生产速率时，consumer会被压垮（overwhelmed），这本质上是一种DOS攻击。相反，在基于pull的系统中，consumer能够根据自身情况调整消费速率。这种方法能够缓和退避协议（backoff protocol）带来的复杂性，退避协议通过consumer反馈是否过载来让broker控制传输速率。基于上述原因，我们选择pull模型。</p>
<p>基于pull系统的另一个优势是适合积累一定的数据后再传给consumer。在基于push的系统中，broker有两个选择：</p>
<ol>
<li>收到一条message就push给consumer</li>
<li>积累一定的message后再push给consumer</li>
</ol>
<p>方法1能够降低延迟但浪费带宽，方法2又无法知道下游的consumer能否立即处理这些批量数据。基于pull的系统能够应付这种情况：从当前offset一次性拉取所有（或设置一个拉取上限）有效数据。因此，这是批量拉取而不会引起高延迟的最佳方法。</p>
<p>原始的基于pull的系统有一个缺点，当broker没有数据时，consumer需要不断轮询是否有数据。为了避免这种忙等待，consumer采用了long poll机制，它会一直阻塞，直到broker积累了指定数量的数据才返回。</p>
<p>你可以想象另一种pull模型，producer把要发送的数据写到本地，然后broker去pull这些数据，最后consumer从broker pull下来。Store-and-forward模型就是这样一种方式（这种模型被用在了ActiveMQ中）。这种模式很有趣，但我们认为它不适用于我们的使用场景，即有上千数量的producer。实验证明，当producer规模很大时，把数据暂存在本地的方法并没有让系统更加可靠，并且运维成本不可估量。实际上可以利用一个pipeline和SLA从而避免producer本地存储。</p>
<h3 id="Consumer-Position"><a href="#Consumer-Position" class="headerlink" title="Consumer Position"></a>Consumer Position</h3><p>记录哪些数据被消费是一个影响消息系统性能的关键点（key performance points）。</p>
<p>很多消息系统把这部分数据存储在broker上。所以，当数据发送给consumer之后，broker要么立即记录这些数据的消费情况，要么等待consumer的响应后记录这些数据的消费情况。这是很符合直觉且务实的一种方法，broker知道哪些数据被消费了，然后将这些数据删除，从而控制数据规模。</p>
<p>但是，要让broker和consumer就哪些数据已经被消费达成一致并不是那么简单。如果broker每次下发数据后就立刻记录已消费情况，且consumer处理这条数据时发生错误（宕机或超时未收到数据等情况），那么这条message将会丢失。为了解决这个问题，许多消息系统增加了确认机制：消息发出后标记为<strong>sent</strong>，broker等待consumer的响应，收到响应后再将消息标记为<strong>consumed</strong>。这一策略修复了数据可能丢失的问题，但产生了一个新问题：</p>
<ol>
<li>consumer在收到数据之后，发送响应之前发生故障，那么这条message会被重复消费</li>
<li>broker必须存储每条message的多个状态，每条message都会执行下列动作：锁定为已发送-发送-标记为已消费-删除，影响性能</li>
</ol>
<p>还有一些其它棘手的问题需要解决，例如message发出后未收到consumer的响应。</p>
<p>Kafka处理这些问题的方法有所不同，我们的topic被拆分成全序的partition，每一个partition在任何时候都只能被一个consumer消费。这意味着每个partition的消费位置仅仅是一个整数，即下一个将要消费message的offset。这使得关于消费情况的状态量非常小，每个partition只是一个数字。这些状态量能够定期的作为checkpointed被存储起来。相比于通过consumer返回响应，这种方法开销更小。</p>
<blockquote>
<p>也就是说，Kafka不会出现consumer争用message的情况，修改offset不需要上锁，减少了系统开销和复杂度。</p>
</blockquote>
<p>这种方法还有一个附带好处，consumer能够回滚offset并重复消费。这有悖于queue的传统概念，但给consumer带来了很多重要的特性。例如，consumer消费了部分数据后发现程序有bug，那么待bug修复后，consumer可以重新消费这些数据。</p>
<h3 id="Offline-Data-Load"><a href="#Offline-Data-Load" class="headerlink" title="Offline Data Load"></a>Offline Data Load</h3><p>可伸缩的持久化（scalable persistence）允许consumer周期性地消费数据并将数据存储在离线系统中，例如Hadoop或关系型数据库。</p>
<p>在Hadoop中，数据的加载可以并行化，加载动作被映射到不同task中，每一个task根据三元组（node，topic，partition）并行地加载数据。Hadoop负责管理task，重启失效的task而不会重复消费——它只是简单地重启task并从最初的offset开始消费。</p>
<blockquote>
<p>这里的意思是，partition的设计能够让Hadoop并行地从broker拉取数据；而持久化的设计能让consumer重新消费过去的数据。</p>
</blockquote>
<h2 id="Message-Delivery-Semantics"><a href="#Message-Delivery-Semantics" class="headerlink" title="Message Delivery Semantics"></a>Message Delivery Semantics</h2><p>现在我们理解了一些关于producer和consumer如何工作的知识，现在我们来讨论一下Kafka在producer和consumer之间提供的语义保证（semantic guarantees）。存在多种消息传递保证（message delivery guarantees）：</p>
<ul>
<li>At most once——消息可能丢，但不会重复递送</li>
<li>At least once——消息绝不会丢，但有可能会重复递送</li>
<li>Exactly once——人们真正想要的，每条消息仅递送一次</li>
</ul>
<p>值得注意的是，这被分解为两个问题：</p>
<ol>
<li>消息发送后的持久化保证（durability guarantees）</li>
<li>消费数据时的保证</li>
</ol>
<p>一些系统声称提供exactly once语义，但阅读细则后发现，这些声称都具有误导性（即他们没有考虑consumer或producer会失效的情况，或者存在多个consumer的情况，或者写到磁盘的数据可能丢失的情况）。</p>
<p>Kafka关于这方面的语义是很直截了当的。当发送message时，message存在被提交（committed）这一概念。一旦message被提交，只要相关的broker依然存活（alive），那么它就不会丢失。存活的定义，以及需要处理哪些类型的失效等内容将在下一节描述。现在我们假设broker是完美的，没有数据丢失，然后试着去理解producer和consumer提供的消息传递保证。如果producer发送一条message时遇到网络故障，它无法确定故障是在message提交之前还是之后发生的。这类似于以autogenerated key方式向数据库插入数据时的语义。</p>
<p>上面描述的不是最强级别的递送语义。即使我们不能确定网络故障时发生了什么，但可以通过让producer产生一个有序的主键（primary key）序列，在故障发生时重试发送request，实现幂等性（idempotent，有了key，producer重复发送同一条message，broker会根据key去重）。在一个server很有可能失效的复制系统中（replicated system），要添加幂等特性并非那么简单。<strong>拥有幂等性之后，producer可以超时重试，直到收到message成功提交的响应，如果是这样的话，发送就能够实现exactly once语义。在未来，Kafka想实现这一功能</strong>。</p>
<p>不是所有的使用场景都需要这么强的保证。对于延迟敏感的使用场景，我们允许producer设置持久化级别。Producer可以等待message被提交再返回，也可以配置为完全异步发送，或者只等待leader保存完message就返回，不必等待同步到follower。</p>
<p>现在我们以consumer的视角看递送语义。所有的replica拥有相同的log和offset，consumer控制消费位置。如果consumer不出故障，那么offset可以始终保存在内存里，但如果consumer失效，我们希望相关partition能够被其它consumer接管，并从正确的位置开始消费。我们假设consumer要读取一些message，那么就如何处理message和如何更新offset，有一些不同的情况：</p>
<ol>
<li>Consumer取到数据，在本地保存offset，然后处理数据。这种情况下，consumer有可能在保存offset之后，保存处理结果之前崩溃（注意这里有两次I/O操作），那么之后的consumer会从保存点继续消费，但这个offset之前的若干message还没有进行处理的。这种情况对应at most once语义，即consumer在崩溃时还有数据未处理，之后也不会被处理了。</li>
<li>Consumer取到数据，处理，最后记录offset。这种情况下，consumer有可能在处理完数据之后，保存offset之前崩溃，那么之后的consumer会读取到部分已处理的数据。这种情况对应at least once语义。其实很多情况下每条message都对应一个主键，所以更新操作是幂等的（收到相同的message，覆盖之前的处理结果即可）。</li>
<li>Exactly once语义。这里的难点不在于服务端的消息系统，而是要就各个consumer看到的offet达成一致，即要保存一个完全正确的offset值。<strong>要达成这一目的，最简单的方法就是在保存offset和保存数据处理结果之间引入两阶段提交协议<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4" target="_blank" rel="external">two-phase commit</a></strong>。但consumer可以通过更加简单的方法来处理，把offset作为处理结果的一部分存储在相同的地方。这种方法要更好一些，因为和consumer交互的后端系统可能不支持两阶段提交协议。作为正面例子，Hadoop ETL系统将打入HDFS的数据和对应的offset都存储在HDFS中，因此保证了数据和offset要么同时保存，要么都不保存。我们和许多数据系统一样，要求这种强语义，实现在message没有主键的情况消除重复（deduplication）数据的读取。</li>
</ol>
<blockquote>
<p>这里的两阶段提交的流程：prepare阶段，consumer（作为协调者）将数据丢给处理系统（作为参与者），处理完后结果返回consumer；commit阶段，consumer根据返回的结果发出commit或rollback请求后再次等待，处理系统会真正将处理完的数据写入磁盘，然后返回一个ack，consumer收到响应后根据响应内容决定是否保存offset。使用两阶段提交协议把因为机器故障而造成不一致的概率降到了较低的水平。</p>
</blockquote>
<p>对于producer来说，Kafka默认是at least once递送语义，用户可以禁用失败重试选项来达到at most once递送语义，consumer也可以在处理数据之前保存offset来达到at most once递送语义。实现exactly-once递送语义一般需要下游存储系统的配合，但Kafka使用的方法更加直接。</p>
<blockquote>
<p>在一个进程和网络都不可靠的系统中，要保证exactly-once语义是不可能的，参考<a href="https://aphyr.com/posts/315-jepsen-rabbitmq" target="_blank" rel="external">这里</a>。</p>
</blockquote>
<h2 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h2><p>Kafka根据配置文件中的设置对topic的partition进行复制，你可以给每个topic设置不同的的复制因子（replication factor）。Replication机制能够实现自动故障转移，使得一台server宕机后消息仍然可用。</p>
<p>其它的消息系统也提供了复制功能，但是，就我们的观点而言，这些都是附加的功能，没有得到广泛应用，其最大的缺点在于：<strong>从机是不活跃的（inactive），吞吐量受到很大影响，需要依赖繁琐的手动配置等（例如ActiveMQ，在shared nothing存储模式下，master挂了之后，需要管理员人工介入启动新master）</strong>。Kafka对replication的默认策略是：topic在创建时replication factor为1，即不进行备份。</p>
<p>复制是以partition作为最小单位的。在没有broker失效的情况下，每一个partition在集群中都有唯一的broker作为leader以及0个或多个broker作为follower。一个partition的备份数量（包括leader上的那份）即备份因子。<strong>客户端的读写请求均通过leader</strong>。通常，partition的数量会大于broker，leader的角色将均匀分配到集群的各个broker上。Follower上的log数据和leader上是相同的——相同的offset以及相同的message顺序，当然，在leader末尾的message还未来得及同步到follow时，两者的数据是不一致的。</p>
<p>Follower会像一个普通的conusmer那样从leader消费数据然后保存到本地log文件。<strong>这样做有一个好处是可以利用consumer的batch特性一次拉取多条message，提高吞吐量</strong>。</p>
<p>Kafka和大多数分布式系统一样，需要自动处理失效的情况，因此需要精确定义节点“存活“的含义。对于Kafka，节点的存活包含两个方面：</p>
<ol>
<li>节点必须维持和ZooKeeper的session（通过ZooKeeper的心跳机制）</li>
<li>Follower必须及时同步leader上的新数据，不能“落后”太多</li>
</ol>
<p>我们把满足以上两点的节点称之为“同步节点”（in sync），以此来把它和“存活”“失效”等概念区分开来。Leader会跟踪记录所有同步节点。如果一个follower挂了或卡住了，或数据同步落后太多，那么leader会把它从同步节点列表中移除，移除条件由配置项<em>replica.lag.time.max.ms</em>来确定。</p>
<p>在分布式系统领域，我们只关注fail/recover类型的失效，即节点突然停止工作并在一段时间后恢复正常。Kafka不会处理拜占庭式的失效，即节点有可能产生任意或恶意的response。</p>
<p>一条message被所有的同步节点同步后（其实是对message所在partition的同步），就可以认为它已经提交了（committed）。只有提交了的message才能交付给consumer端。这意味着consumer不必担心由于leader失效可能导致的message丢失。另一方面，为了在延迟和持久化之间进行权衡，producer可以选择是否等待message提交。Producer选择哪一种策略是根据配置项<em>request.required.acks</em>来确定的。</p>
<p>Kafka保证只要有至少一个同步节点存活，已提交的message就不会丢失。</p>
<blockquote>
<p>这里只是说提交成功的message不会丢失，并没有说producer发送成功的message不会丢失</p>
</blockquote>
<p>Kafka在短时间的失效转移时间内仍然可用，但网络分区的情况下不保证始终可用，<a href="https://aphyr.com/posts/293-jepsen-kafka" target="_blank" rel="external">Kafka选择了CA而牺牲了P，但一旦出现网络分区，还是会牺牲可用性</a>。</p>
<blockquote>
<p>个人认为，当发生分区时，CA会最终转换成CP或AP。例如，当出现网络分区，producer会因为发送失败返回错误，这就不是100%可用了。因此，分布式系统必须要具备分区容忍性。</p>
</blockquote>
<h3 id="Replicated-Logs-Quorums-ISRs-and-State-Machines"><a href="#Replicated-Logs-Quorums-ISRs-and-State-Machines" class="headerlink" title="Replicated Logs: Quorums, ISRs, and State Machines"></a>Replicated Logs: Quorums, ISRs, and State Machines</h3><p>Kafka partition的核心是复制log（replicated log）。在分布式数据系统中，log复制是最为重要的概念之一，实现方法也非常多。Log复制可以被应用于很多分布式系统中，作为复制状态机概念的一部分。</p>
<p>一个要复制的log可以看做是一个处理模型，目的是通过协商使一个序列内的元素（log entry）顺序达成一致，即log entry按顺序0,1,2…排列。实现这一目的的方法很多，最为简单且快速的方法是：系统中有一个leader，由它来决定log entry的排列顺序。只要leader存活，所有follower只需要按照这个顺序将log entry拷贝过来就可以了，即log entry的顺序在所有节点上达成了一致。</p>
<p>如果leader不挂，我们当然可以不需要follower。但如果leader挂了，我们需要从follower中选出一个新的leader。但follower本身可能会落后leader很多，也可能会挂，所以我们要确保选出一个最新（up-to-date）的follower。对于log复制算法，Kafka最基本的保证是：当Kafka告知client一个message已经被提交，并且leader挂了，新选出的leader必须存有这条message。这里需要进行权衡：如果leader需要等待越多的follower返回message提交成功，那么就有更多的follower有资格成为leader，但延迟会提高。</p>
<p>如果选择一个follower的数量，这些follower和leader上log的内容相同或是leader的子集，那么这个数就叫做法定人数(<a href="https://en.wikipedia.org/wiki/Quorum_(distributed_computing" target="_blank" rel="external">Quorum</a>))。</p>
<p>一种权衡一致性和延迟这两方面的算法是：对于确定决议和leader选举，采用大多数选票（majority vote）的方法，少数服从多数。<strong>Kafka没有这样做，但这种方法提供了一种如何进行权衡的思路</strong>。假设我们有2f+1个备份，那么当满足如下条件时，leader宕机后新选举出的leader能够保证拥有所有已经被提交的数据：</p>
<ol>
<li>leader宣布数据已经提交之前，f+1个备份已经成功接收了这份数据</li>
<li>选举新leader时，从至少为f+1（和上面的f+1不是同一个集合）个备份中选出拥有最完整log的备份成为新leader</li>
<li>不超过f个备份节点失效</li>
</ol>
<p>因为在不多于f个备份节点失效的情况下，任何f+1个备份节点中一定存在至少一个备份节点包含所有已经提交的数据（可以考虑极端情况下，f+1个成功接收的备份节点中有f个失效，仍然会有最后一个节点保存有完整的数据）。这个节点上的log是最完整的，因此它会被选为新leader。这些算法还有很多细节需要处理（例如需要精确定义如何让log更加完整，确保在leader失效或服务器数量发生变化时log的一致性），但我们现在忽略这些细节。</p>
<p>这种获得大多数选票获胜的算法有一个很好的特性：延迟只依赖于那些最快的服务器。也就是说，如果备份因子为3，那么延迟由最快的slave决定，而不是最慢的。</p>
<p>这类算法有许多，包括ZooKeeper的Zab，Raft，Viewstamped Replication。我们认为和Kafka最接近的算法是微软的PacificA。</p>
<p>Majority vote算法的缺点是它无法处理多个节点失效导致没有可以选择的候选leader这种情况。容忍1个节点失效需要3份拷贝，容忍2个节点失效需要5份拷贝。在我们的经验中，单纯的增加冗余来对应单点失效是不够的，但5倍次数的写，5倍存储空间和1/5的吞吐量在处理大数据时又是不太实际的。</p>
<blockquote>
<p>这里的大概意思是：如果采用majority vote策略，增加拷贝还是不能容忍大多数节点失效这种情况；而要能容忍大多数节点失效，就需要每次write都同步复制到其它从节点，而这种同步复制的效率又是不能接受的。</p>
</blockquote>
<p>这可能就是为什么quorum算法常常出现在存储元数据的系统中，例如ZooKeeper，而很少出现在原始数据的存储中。例如HDFS的namenode的高可用性就是以<a href="http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1/" target="_blank" rel="external">majority vote</a>作为基础，但它并没有用在datanode中。</p>
<p>Kafka选择了采用了稍微不同的方法来选择quorum集合。不同于majority vote，Kakfa动态地维护一个叫做in-sync replicas（ISR）的集合，集合内的节点去追赶复制leader。只有这个集合中的成员才有资格选举为leader。<strong>只有所有ISR都接收了数据，才认为数据被提交成功</strong>。ISR集合保存在ZooKeeper中，动态变化，所以当需要选举leader时，ISR中的成员是有资格成为leader的。对于Kafka的使用模型来说，这是很重要的一点，Kafka的特点是有多个partition并且leader在集群中的分布是非常平衡的。通过ISR模型和f+1个备份，Kafka可以在f个节点失效的情况下不丢失已提交的数据。</p>
<blockquote>
<p>从上面的描述可以看出，Kafka其实也采用了quorum-based技术，只是确定quorum集合的方法和常规的majority vote有所不同，majority vote把备份的大多数作为法定人数，而ISR技术确定法定人数是根据ISR数量确定的。</p>
</blockquote>
<p>对于我们关注的许多场景，这样一种权衡的做法是合理的。实际上，为了容忍f个节点失效，majority vote算法和ISR策略都需要等待f+1个备份节点返回确认才算提交成功（例如，如果需要容忍1个节点失效，majority vote算法需要3个备份节点和至少1个备份节点返回确认，而ISR算法只需要2个备份节点和至少1个备份节点的返回确认）。<strong>Majority vote的优势在于提交操作不依赖于最慢的那些服务器</strong>。但是，我们认为让用户选择在提交操作时是否阻塞更加合理，并且用更小的备份因子换取更大的吞吐量和更小的磁盘空间是很划算的。</p>
<p>Kafka另一个独有设计是，它不要求失效的节点在恢复后拥有完好无损的全量数据。这有别于一般的复制算法稳定存储（stable storage）的思想，这些复制算法不允许故障恢复后出现丢失数据或是不一致的情况。但这一思想有两个问题：  </p>
<ol>
<li>在持久化存储系统中，最常见的就是磁盘错误，而磁盘错误经常时数据无法保持完整无缺  </li>
<li>即使1不是问题，我们也不想为了保证一致性而每次写操作都调用fsync，这会使性能降低2到3个数量级<br>Kafka的协议允许备份节点恢复工作后重新加入ISR，加入之前必须全部重同步所有的数据。<blockquote>
<p>这里的意思应该是，为了效率，同步到follower的数据不会立刻刷到磁盘，而是保存在内存中，但这样就会存在宕机丢失数据的风险，Kafka通过失效重启后需要跟leader同步，恢复未刷入磁盘的数据。</p>
</blockquote>
</li>
</ol>
<h3 id="Unclean-leader-election-What-if-they-all-die"><a href="#Unclean-leader-election-What-if-they-all-die" class="headerlink" title="Unclean leader election: What if they all die?"></a>Unclean leader election: What if they all die?</h3><p>Kafka在数据持久化方面的的保证是，至少存在一个replica在ISR中。如果一个partition的所有replica都挂了，那么这样的保证也就不存在了。</p>
<p>但是，一个实际的系统需要在所有replica都挂了之后做某些合理的事情。如果这样的事情不幸发生，考虑之后会发生什么是很重要的。具体的实现可以有两种：</p>
<ol>
<li>不允许unclean leader election，等待一个拥有全量数据并且属于ISR的replica恢复并将它选为leader</li>
<li>允许unclean leader election，选择第一个恢复的replica（不一定属于ISR）作为leader</li>
</ol>
<p>这是一个在可用性和一致性之间的权衡。如果我们等待ISR中的replica，那么只要没有replica恢复，服务将始终不可用。如果这些replica已经损坏或数据已经丢失，那么服务将永久失效。另一方面，如果一个不属于ISR的非同步（non-in-sync）replica恢复正常，然后我们允许它成为leader，那么它的log数据将作为信息源（source of truth），即使它不保证拥有每一条已提交数据。<strong>在当前的版本中我们选择第二种策略，即当ISR的所有节点都宕机时，更倾向于数据的不一致</strong>。未来，我们会让这些策略变成可配置的，以此来适应不同的场景。</p>
<p>这一窘境不只是在Kafka中出现，它也出现在以quorum为基础的模式中。例如majority voting模式，如果大多数服务器失效，那么数据不保证100%完整，一致性就有可能被破坏。</p>
<h3 id="Availability-and-Durability-Guarantees"><a href="#Availability-and-Durability-Guarantees" class="headerlink" title="Availability and Durability Guarantees"></a>Availability and Durability Guarantees</h3><p>当向Kafka发数据时，producer可以选择是否等待0个、1个或所有（-1）replica的回应。注意，等待所有replica的回应只是等待IRS收到数据而不是所有的replica都收到数据。默认的，当<em>request.required.acks</em>=-1时，只要当前ISR集合收到数据，就会返回响应。例如，如果一个topic的备份因子是2，一个replica挂了（ISR内节点的数量为1），那么<em>request.required.acks</em>=-1时写操作能成功。但是，如果剩下的replica也挂了，那么发送的数据将会丢失。虽然这确保了系统的可用性，但对于更加关注持久性的用户来说，这种策略是不合适的。因此，我们提供了关于可用性和持久性两方面的topic级别的两个配置。</p>
<ol>
<li>禁止unclean leader election（配置项<em>unclean.leader.election.enable</em>）。如果所有的replica不可用，那么这个partition将会变得不可用，直到ISR中的成员恢复正常。这一策略更倾向于数据的持久性，而不是可用性。</li>
<li>指定一个最小的ISR规模（配置项<em>min.insync.replicas</em>）。写入的数据只有被这个数量以上的replica接收才被认为是提交成功。这能够防止写入单个replica而可能造成的数据丢失。这一设置只有在producer配置了<em>required.acks</em>=-1时生效，它保证了一条message至少会有这么多数量的ISR成员回应ack。这一设置权衡了一致性和可用性。<em>min.insync.replicas</em>设置的越大，保证能够写入的replica越多，丢失数据的风险也越小，因此具有更好的一致性。但是，它降低了可用性。因为写入数据时，如果ISR数量小于<em>min.insync.replicas</em>，那么partition将变得不可用。</li>
</ol>
<h3 id="Replica-Management"><a href="#Replica-Management" class="headerlink" title="Replica Management"></a>Replica Management</h3><p>上面讨论的关于replicated log的内容只针对单个log，即一个topic的一个partition。但一个Kafka集群可能要管理成百上千的partition，我们会采用轮询（round-robin）的方式均匀分布一个集群中的partition，避免许多都partition都堆积在小部分节点上。同样，leader的分配也做到均匀，使得每个节点只成为部分partition的leader。</p>
<p>Leader选举时会有一段时间的不可用，对这段时间的优化也是很重要的。最原始的实现是，当leader挂掉后，拥有此partition的所有节点都要开始选举过程。<strong>作为替代，我们选择一台broker作为controller。Controller负责检测broker是否失效，当检测到失效后，它会将受影响的partition（这些partition的leadership在这台失效broker上）的leadership转给其它broker。这样做的结果是，Kafka能够处理大量的leadership转移的需求，对于partition很多的情况，选举过程也能做到更轻、更快</strong>。如果Controller挂了，会选一台存活的broker作为新Controller。</p>
<h2 id="Quotas"><a href="#Quotas" class="headerlink" title="Quotas"></a>Quotas</h2><p>从0.9开始，Kafka可以设置生产和消费的配额限制。配额以client id为单位设置比特率阈值。一个client id逻辑上表示一个独立应用程序，因此一个client id可以建立多个producer和consumer实例，那么配额机制将会把这些实例作为一个整体统一分配配额。例如，为client id为”test-client”的程序分配生产配额10MB/sec，那么这个配额将会被程序中的所有producer实例共享。</p>
<h3 id="Why-are-quotas-necessary"><a href="#Why-are-quotas-necessary" class="headerlink" title="Why are quotas necessary?"></a>Why are quotas necessary?</h3><p>Producer和consumer有可能产生非常高的数据流量，垄断了broker上的资源，导致网络饱和并且拒绝其它客户端的请求。配额机制能够有效防止这一问题，尤其是对于大型multi-tenant集群，这样的集群可能由于小部分客户端的不良行为降低整体服务质量，影响到其它正常用户。实际上，当Kafka作为一个服务运行时，甚至可以根据某些协商限制对的API使用。</p>
<h3 id="Enforcement"><a href="#Enforcement" class="headerlink" title="Enforcement"></a>Enforcement</h3><p>默认的，每一个单独的client id接收由集群设置的固定配额，单位是bytes/sec（quota.producer.default, quota.consumer.default）。这个配额是针对每一个broker的，即一个客户端能够向一个broker发送或接收最大速度为X bytes/sec的数据。我们把配额对应到broker而不是cluster是因为后者需要在broker间动态共享一个client的配额使用情况，而这比实现一个配额机制本身要困难。</p>
<p>当broker检测到有配额超额会有什么反应？我们没有返回错误，而是尝试降低超额客户端的速率。Kafka计算应该延迟多长时间返回response，使速率达到阈值以下。这种方法让客户端感知不到配额超额的存在（客户端不需要处理超额情况），即不用实现额外的逻辑去回退（backoff）并重试（返回错误的话，客户端需要先回退，再重试），增加实现难度。如果客户端真的先回退，再重试，那么速率将更加难以控制，这与配额机制的初衷截然相反。</p>
<p>为了快速并且准确的检测是否有配额超标，Kafka根据一些小的窗口来测量速率（例如，每1秒钟有30个窗口）。通常更大测量窗口（例如，每30秒有10个窗口）导致了更大的数据量和超标后的更长的延迟，就用户体验而言，这是不好的。</p>
<h3 id="Quota-overrides"><a href="#Quota-overrides" class="headerlink" title="Quota overrides"></a>Quota overrides</h3><p>如果需要更高或者更低的配额，我们可以覆盖默认值，配置后的值将会写入Zookeeper的/config/clients节点。这个值能够立即被所有的broker读取，这使得我们可以改变配额而不需要重启集群。</p>
<h2 id="Network-Layer"><a href="#Network-Layer" class="headerlink" title="Network Layer"></a>Network Layer</h2><p>系统的网络层相当直截了当，使用了NIO，详细描述<a href="http://www.jianshu.com/p/ff1432f5a14b" target="_blank" rel="external">看这里</a>。Sendfile的实现是通过将<em>MessageSet</em>接口传递给<em>writeTo</em>方法来完成的，这使得文件中的message set能够通过更加高效的<em>transferTo</em>方法传输，而不用在进程buffer中执行写操作。线程模型为一个线程接受新连接，N个线程处理固定数量的连接，这一设计已经经过彻底的测试，并且实现起来较为简单快速。协议也是非常简单的，这允许将来更多的语言去实现客户端。</p>
<h2 id="Log"><a href="#Log" class="headerlink" title="Log"></a>Log</h2><h3 id="Guarantees-1"><a href="#Guarantees-1" class="headerlink" title="Guarantees"></a>Guarantees</h3><p>Log数据有一个配置参数M，表示数据刷入磁盘之前可以缓存message的最大数目。Kafka启动时，一个log恢复线程会遍历所有最新的segment文件，验证这些文件中的每一条message是否有效。如果一条message的起始offset与message大小之和小于文件大小并且message中有效载荷（payload）的CRC32等于message中保存的CRC校验值，那么这条message就是有效的。如果发现message损坏（corruption）了，那么log文件就会被截断到有效的offset。</p>
<p>Kafka需要处理两类数据损坏：</p>
<ol>
<li>未写入磁盘块的数据因为机器故障而丢失</li>
<li>无效的数据被加入到了log文件中</li>
</ol>
<p>出现这些问题的原因是因为操作系统通常不保证写inode和写实际磁盘块这两个操作的先后顺序，因此除了可能丢失写入的数据，还有可能出现无效的数据，因为在inode更新之后，磁盘块写入数据之前，机器有可能崩溃。CRC校验就能检测到这种极端情况，从而阻止log文件被破坏。</p>
<blockquote>
<p>Linux文件系统的inode中有一个指针数组，里面的指针直接或间接指向存储实际数据的磁盘块，具体细节<a href="https://en.wikipedia.org/wiki/Inode_pointer_structure" target="_blank" rel="external">看这里</a>。当指针指向实际数据块，但数据写入对应数据块之前崩溃，那么这些指针指向的数据就是无效的数据，这就是第二类数据损坏。</p>
</blockquote>
<h2 id="Distribution-1"><a href="#Distribution-1" class="headerlink" title="Distribution"></a>Distribution</h2><h3 id="Consumer-Offset-Tracking"><a href="#Consumer-Offset-Tracking" class="headerlink" title="Consumer Offset Tracking"></a>Consumer Offset Tracking</h3><p>High-level的consumer追踪记录它消费的每个partition的最大offset并且周期性地提交offset，使得它可以在重启后继续从这些offset开始消费。Kafka提供了一个配置项，对于一个group中的所有offset，都会存在一台指定的broker上，这台broker称为<em>offset manager</em>，即这个group中的所有consumer应该提交offset到这台broker，或者从这台broker获取offset。High-level的consumer应该自动处理这些逻辑。如果你使用simple consumer，则需要手动管理offset。Consumer可以发送<em>GroupCoordinatorRequest</em>请求到任意broker来寻找它的offset manager，响应<em>GroupCoordinatorResponse</em>将会包含offset manager的编号，之后consumer便能通过这个offset manager来commit或fetch offset了。如果offset manager转移到了其它broker，那么broker需要重新寻找offset manager。如果你想要手动管理你的offset，可以看<a href="https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka" target="_blank" rel="external">这里</a>。</p>
<p>当offset manager收到一个<em>OffsetCommitRequest</em>，它会把这个request compacted到一个名为<em>__consumer_offsets</em>的topic里。<strong>当这个topic的所有replica都接收到这个提交的offset之后，<em>offset manager</em>才会响应客户端成功提交了offset。</strong>万一offset在复制过程中超时了，那么提交offset的操作将会失败，consumer将会重试提交offset（这些都由high-level consumer自动完成）。Broker周期性地压缩保存offset的topic，因为它只需要保存每个partition最近提交的offset。同时，offset manager也会在内存中缓存offset以便client能够快速的获取。</p>
<p>当offset manager收到一个offset fetch的请求，它只是返回保存在内存中的最近一次提交的offset。如果offset manager刚刚启动，或者刚刚成为某些group的offset manager（成为<em>__consumer_offsets</em>对应partition的新leader），那么它需要从磁盘加载offset到内存。在这个过程中，客户端的offset fetch请求将会失败，那么它需要重新发送<em>OffsetFetchRequest</em>请求（这一过程由high-level consumer自动完成）。</p>
<h2 id="Hardware-and-OS"><a href="#Hardware-and-OS" class="headerlink" title="Hardware and OS"></a>Hardware and OS</h2><h3 id="Application-vs-OS-Flush-Management"><a href="#Application-vs-OS-Flush-Management" class="headerlink" title="Application vs. OS Flush Management"></a>Application vs. OS Flush Management</h3><p>Kafka总是立即将所有的数据写入文件系统，但写入磁盘的策略是可以设置的，使得Kafka可以控制内核缓存（页缓存）数据刷入磁盘的时机。策略包括：</p>
<ol>
<li>一段时间之后强制刷入</li>
<li>页缓存中积累了一定数量的message后强制写入</li>
</ol>
<p>Kafka最终一定会通过fsync以确保数据被刷入磁盘。当崩溃重启时，没有通过fsync而存在于磁盘的message，需要对它们的有效性进行检验，方式为CRC校验，并且修复对应的索引文件。</p>
<p>值得注意的是，Kafka的持久化特性可以不依赖于数据同步到磁盘，当节点失效重启后可以通过replica同步数据。</p>
<p>我们建议使用默认的刷磁盘设置，即禁止应用程序调用fsync（Kafka的<em>flush.messages</em>配置和<em>flush.ms</em>配置默认是关闭的）。这意味着我们只依赖操作系统的后台flush机制和Kafka默认的flush机制就可以了，这在大多数使用场景下都能满足我们的需求：不去调整这些参数，能够带来更好的吞吐量和延迟，还有完整恢复的保证（full recovery guarantees），也就是数据的完整性。在数据完整性方面，我们认为通过复制机制比强制刷磁盘的方法要好一些。不过应用层使用fsync也是允许的。</p>
<blockquote>
<p>注意，flush和fsync不是一个操作，具体细节<a href="http://stackoverflow.com/questions/2340610/difference-between-fflush-and-fsync" target="_blank" rel="external">看这里</a>。</p>
</blockquote>
<p>Flush操作可以被调节有一个缺点，就是降低了磁盘的效率，即操作系统不能更好的对写操作进行重排序（re-order）。这样做会引入延迟，就像在大多数Linux文件系统中fsync会阻塞写操作并带来延迟一样，而后台的flush操作具有更加细粒度的页级别（page-level）的锁。</p>
<blockquote>
<p>总之，就是不要轻易修改flush相关的配置。</p>
</blockquote>
<p>一般情况下，你不需要调节文件系统层面的配置，但接下来我们将会重温一下这方面的内容。</p>
<h3 id="Understanding-Linux-OS-Flush-Behavior"><a href="#Understanding-Linux-OS-Flush-Behavior" class="headerlink" title="Understanding Linux OS Flush Behavior"></a>Understanding Linux OS Flush Behavior</h3><p>在Linux中，写入文件系统的数据是先保存在<a href="https://en.wikipedia.org/wiki/Page_cache" target="_blank" rel="external">页缓存</a>中的，之后才会写入磁盘（通过应用层的fsync系统调用或者操作系统自己的刷盘策略）。刷数据到磁盘是通过后台线程pdflush完成的。</p>
<p>Pdflush有一个配置，可以设置允许多少脏数据保存在页缓存中，或者多长时间之后将数据刷入磁盘，<a href="http://www.westnet.com/~gsmith/content/linux-pdflush.htm" target="_blank" rel="external">这里</a>有详细描述。当pdflush刷数据操作的速率跟不上应用程序写入的速率时，最终会导致写操作被阻塞。</p>
<p>你可以通过下面的命令来观察操作系统内存使用情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/meminfo</span><br></pre></td></tr></table></figure>
<p>结果中每一项的含义在上面的链接中给出。</p>
<p>当要把数据写入磁盘时，使用页缓存和使用进程内缓存相比，有一些好处：</p>
<ol>
<li>I/O调度器会把连续的小量数据的写合并为一个大量数据的写，提高吞吐量；</li>
<li>I/O调度器会尝试对写操作重排序，减少磁头移动，提高吞吐量；</li>
<li>能够自动利用空闲内存。</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/04/17/hello/" rel="next" title="First Blood">
                <i class="fa fa-chevron-left"></i> First Blood
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/04/22/ActiveMQ/" rel="prev" title="ActiveMQ">
                ActiveMQ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpeg"
               alt="Ouyang Liduo" />
          <p class="site-author-name" itemprop="name">Ouyang Liduo</p>
          <p class="site-description motion-element" itemprop="description">每个认真生活的人，都值得被认真对待。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">8</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Topics-and-Logs"><span class="nav-number">1.1.</span> <span class="nav-text">Topics and Logs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distribution"><span class="nav-number">1.2.</span> <span class="nav-text">Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Producers"><span class="nav-number">1.3.</span> <span class="nav-text">Producers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumers"><span class="nav-number">1.4.</span> <span class="nav-text">Consumers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Guarantees"><span class="nav-number">1.5.</span> <span class="nav-text">Guarantees</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">2.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Persistence"><span class="nav-number">3.</span> <span class="nav-text">Persistence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Don’t-fear-the-filesystem"><span class="nav-number">3.1.</span> <span class="nav-text">Don’t fear the filesystem!</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Constant-Time-Suffices"><span class="nav-number">3.2.</span> <span class="nav-text">Constant Time Suffices</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Efficiency"><span class="nav-number">4.</span> <span class="nav-text">Efficiency</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#End-to-end-Batch-Compression"><span class="nav-number">4.1.</span> <span class="nav-text">End-to-end Batch Compression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Producer"><span class="nav-number">5.</span> <span class="nav-text">The Producer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Load-balancing"><span class="nav-number">5.1.</span> <span class="nav-text">Load balancing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asynchronous-send"><span class="nav-number">5.2.</span> <span class="nav-text">Asynchronous send</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Consumer"><span class="nav-number">6.</span> <span class="nav-text">The Consumer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Push-vs-pull"><span class="nav-number">6.1.</span> <span class="nav-text">Push vs. pull</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumer-Position"><span class="nav-number">6.2.</span> <span class="nav-text">Consumer Position</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Offline-Data-Load"><span class="nav-number">6.3.</span> <span class="nav-text">Offline Data Load</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Message-Delivery-Semantics"><span class="nav-number">7.</span> <span class="nav-text">Message Delivery Semantics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replication"><span class="nav-number">8.</span> <span class="nav-text">Replication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Replicated-Logs-Quorums-ISRs-and-State-Machines"><span class="nav-number">8.1.</span> <span class="nav-text">Replicated Logs: Quorums, ISRs, and State Machines</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unclean-leader-election-What-if-they-all-die"><span class="nav-number">8.2.</span> <span class="nav-text">Unclean leader election: What if they all die?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Availability-and-Durability-Guarantees"><span class="nav-number">8.3.</span> <span class="nav-text">Availability and Durability Guarantees</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Replica-Management"><span class="nav-number">8.4.</span> <span class="nav-text">Replica Management</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quotas"><span class="nav-number">9.</span> <span class="nav-text">Quotas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-are-quotas-necessary"><span class="nav-number">9.1.</span> <span class="nav-text">Why are quotas necessary?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Enforcement"><span class="nav-number">9.2.</span> <span class="nav-text">Enforcement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quota-overrides"><span class="nav-number">9.3.</span> <span class="nav-text">Quota overrides</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Network-Layer"><span class="nav-number">10.</span> <span class="nav-text">Network Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Log"><span class="nav-number">11.</span> <span class="nav-text">Log</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Guarantees-1"><span class="nav-number">11.1.</span> <span class="nav-text">Guarantees</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distribution-1"><span class="nav-number">12.</span> <span class="nav-text">Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumer-Offset-Tracking"><span class="nav-number">12.1.</span> <span class="nav-text">Consumer Offset Tracking</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hardware-and-OS"><span class="nav-number">13.</span> <span class="nav-text">Hardware and OS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Application-vs-OS-Flush-Management"><span class="nav-number">13.1.</span> <span class="nav-text">Application vs. OS Flush Management</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-Linux-OS-Flush-Behavior"><span class="nav-number">13.2.</span> <span class="nav-text">Understanding Linux OS Flush Behavior</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ouyang Liduo</span>
</div>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




	




  
  

  

  

  

  


</body>
</html>
